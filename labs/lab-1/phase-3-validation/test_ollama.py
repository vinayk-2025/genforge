"""
test_ollama.py

Purpose:
    Demonstrate how to run a local Ollama model (tinyllama) from Python using subprocess.
    Sends a simple prompt and prints the model‚Äôs response.

Prerequisites:
    1. Install Python 3.10+.
    2. Install Ollama (local LLM runner):
         - Windows: https://ollama.com/download
         - macOS:   brew install ollama
         - Linux:   follow instructions at https://ollama.com/download
    3. Verify Ollama installation:
         ollama --version
    4. Pull the required model (tinyllama in this example):
         ollama pull tinyllama
    5. Ensure Ollama service is running:
         ollama serve

Usage:
    1. Run the script:
         python test_ollama.py

    2. Expected output:
         üîç Sending prompt to 'tinyllama' via Ollama...

         ‚úÖ Ollama responded:
         (Model‚Äôs generated answer, e.g. "Hello GenAI Agentic Course students!")

Notes:
    - Uses subprocess to call Ollama CLI directly.
    - Includes error handling for:
         ‚Ä¢ Ollama not installed or not in PATH
         ‚Ä¢ Timeout if Ollama does not respond
         ‚Ä¢ Other unexpected errors
    - Students can edit the `prompt` variable to experiment with different inputs.
"""

import subprocess

def run_ollama_hello_world():
    """Send a simple prompt to Ollama and print the response."""
    prompt = "Say hello to the GenAI Agentic Course students."
    print("üîç Sending prompt to 'tinyllama' via Ollama...\n")

    try:
        # Launch Ollama process with the tinyllama model
        process = subprocess.Popen(
            ["ollama", "run", "tinyllama"],
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            encoding="utf-8"
        )

        # Send the prompt and capture output
        stdout, stderr = process.communicate(input=prompt, timeout=30)

        # Check exit code and print result
        if process.returncode == 0:
            print("‚úÖ Ollama responded:")
            print(stdout.strip())
        else:
            print("‚ùå Ollama returned an error:")
            print(stderr.strip())

    except subprocess.TimeoutExpired:
        print("‚ö†Ô∏è Ollama timed out. Try running manually.")
    except FileNotFoundError:
        print("‚ùå Ollama is not installed or not in PATH.")
    except Exception as e:
        print(f"‚ö†Ô∏è Unexpected error: {e}")

if __name__ == "__main__":
    run_ollama_hello_world()

