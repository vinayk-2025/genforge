---
title: "Glossary â€“ Existing AI Concepts"
description: "Foundational terminology for GenAI, LLMs, neural networks, and core machine learning techniques"
date: 2025-11-10
author: Satya Prakash Nigam
tags: [GenAI, LLM, NLP, Deep Learning, Neural Networks, AI Glossary]
layout: post
permalink: /glossary/glossary-existing-concepts/
---

# Glossary â€“ Existing AI Concepts
**Reference for Lecture 3B and 4A**

This glossary defines foundational concepts in artificial intelligence and machine learning. These terms support your understanding of GenAI systems, model architecture, and training workflows.

---

## Core Terminology

| Term                         | Description                                                                 |
|------------------------------|-----------------------------------------------------------------------------|
| Generative AI                | AI systems that generate text, images, video, audio, or other media in response to prompts |
| Large Language Model (LLM)   | AI model trained on vast text data to understand and generate human-like language |
| Neural Network               | Framework of interconnected nodes (neurons) used to process and interpret data |
| Explainability               | Techniques that make AI model decisions interpretable and understandable to humans |
| Transfer Learning            | Reusing a pre-trained model and fine-tuning it for a specific task or domain |
| Attention Mechanism          | Technique that enables models to focus on the most relevant parts of input data |
| Reinforcement Learning       | Learning paradigm where an AI agent improves performance through trial and error |
| Natural Language Processing (NLP) | Study of interaction between computers and human language |
| Deep Learning                | Subset of ML using multi-layered neural networks to analyse vast amounts of data |
| Generative Adversarial Network (GAN) | Machine learning model using two networks to generate realistic synthetic data |

---

## Student Reflection Prompts

- What distinguishes LLMs from traditional rule-based NLP systems?
- How does transfer learning improve model efficiency?
- Why is explainability important in high-stakes domains like healthcare or finance?
- What role does attention play in transformer-based models?
- How do GANs differ from other generative models?

---

## Instructor Notes

This glossary supports Lecture 3B and 4A. Encourage students to revisit these terms before exploring model families, training workflows, and multimodal capabilities. Use visual demos to reinforce attention, GANs, and transfer learning.

---

Curated by **Satya Prakash Nigam**  
Independent AI Consultant Â· Fractional CTO Â· Product Architect Â· Technical Enablement Strategist  
ğŸŒ Personal: [spnigam.in](https://spnigam.in)  
ğŸ§ª Platform: [aialchemyhub.in](https://www.aialchemyhub.in)  
ğŸ“º YouTube: [AI Alchemy Hub](https://www.youtube.com/@AIAlchemyHub-zx6lz)  
ğŸ’¬ Community (Coming Soon): [community.aialchemyhub.in](https://community.aialchemyhub.in)  
ğŸ’¬ Zulip: [aialchemyhub.zulipchat.com](https://aialchemyhub.zulipchat.com)  
ğŸ“§ Email: spnigam25@yahoo.com  
ğŸ”— LinkedIn: [linkedin.com/in/spn25](https://www.linkedin.com/in/spn25)  
ğŸ’» GitHub: [github.com/satya25](https://github.com/satya25)  
ğŸ¤– Hugging Face: [huggingface.co/satya25](https://huggingface.co/satya25)

---
